{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "\n",
    "### Student Name: Matthew Block\n",
    "[Web Scraping Repository](https://github.com/matthewpblock/620-mod6-web-scraping/)  \n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching article from: https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/\n",
      "Parsing HTML content...\n",
      "Article content found. Processing content...\n",
      "Saving extracted article to laser_headlights_article.html...\n",
      "Successfully saved the article to 'laser_headlights_article.html'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def extract_and_save_article(url: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    Fetches an article from a URL, extracts the main article content using BeautifulSoup,\n",
    "    and saves it to an HTML file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the web page containing the article.\n",
    "        output_filename (str): The name of the file to save the extracted article to.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching article from: {url}\")\n",
    "    try:\n",
    "        # Using a user-agent can help avoid being blocked by some sites\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Parsing HTML content...\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the main article tag. Based on inspection of Hackaday pages,\n",
    "    # the main content is within an <article> tag.\n",
    "    article_tag = soup.find('article')\n",
    "\n",
    "    if not article_tag:\n",
    "        print(\"Could not find the <article> tag on the page.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Article content found. Processing content...\")\n",
    "\n",
    "    # Convert relative URLs to absolute URLs for links, images, and scripts\n",
    "    for tag in article_tag.find_all(['a', 'link'], href=True):\n",
    "        tag['href'] = urljoin(url, tag['href'])\n",
    "\n",
    "    for tag in article_tag.find_all(['img', 'script', 'video', 'audio', 'source'], src=True):\n",
    "        tag['src'] = urljoin(url, tag['src'])\n",
    "\n",
    "    # Extract the title from the article header for the new HTML's <title> tag\n",
    "    title_text = \"Extracted Article\"\n",
    "    title_tag = article_tag.find('h1', class_='entry-title')\n",
    "    if title_tag:\n",
    "        title_text = title_tag.get_text(strip=True)\n",
    "\n",
    "    # Use prettify() to get a nicely formatted string of the article tag\n",
    "    article_html = article_tag.prettify()\n",
    "\n",
    "    # Create a complete HTML document for the output\n",
    "    # I've added some basic CSS for better readability\n",
    "    output_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{title_text}</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen-Sans, Ubuntu, Cantarell, \"Helvetica Neue\", sans-serif;\n",
    "            line-height: 1.6;\n",
    "            color: #333;\n",
    "            background-color: #fdfdfd;\n",
    "            max-width: 800px;\n",
    "            margin: 20px auto;\n",
    "            padding: 0 20px;\n",
    "        }}\n",
    "        h1, h2, h3, h4, h5, h6 {{\n",
    "            line-height: 1.2;\n",
    "        }}\n",
    "        a {{\n",
    "            color: #0073aa;\n",
    "        }}\n",
    "        img, video {{\n",
    "            max-width: 100%;\n",
    "            height: auto;\n",
    "            display: block;\n",
    "            margin: 1em 0;\n",
    "        }}\n",
    "        pre {{\n",
    "            background-color: #f0f0f0;\n",
    "            padding: 1em;\n",
    "            overflow-x: auto;\n",
    "            border-radius: 4px;\n",
    "        }}\n",
    "        code {{\n",
    "            font-family: \"Courier New\", Courier, monospace;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    {article_html}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Saving extracted article to {output_filename}...\")\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(output_html)\n",
    "        print(f\"Successfully saved the article to '{output_filename}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file {output_filename}: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "# The URL from the request\n",
    "TARGET_URL = \"https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/\"\n",
    "OUTPUT_FILE = \"laser_headlights_article.html\"\n",
    "\n",
    "\n",
    "extract_and_save_article(TARGET_URL, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Laser Headlights Work 130 Comments by: Lewin Day March 22, 2021 When we think about the onward march of automotive technology, headlights aren’t usually the first thing that come to mind. Engines, fuel efficiency, and the switch to electric power are all more front of mind. However, that doesn’t mean there aren’t thousands of engineers around the world working to improve the state of the art in automotive lighting day in, day out. Sealed beam headlights gave way to more modern designs once r\n"
     ]
    }
   ],
   "source": [
    "parser = 'html5lib'\n",
    "html_file = 'laser_headlights_article.html'\n",
    "\n",
    "try:\n",
    "    # Open and read the file\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # First, find the <article> tag\n",
    "    article_tag = soup.find('article')\n",
    "\n",
    "    if article_tag:\n",
    "        # Now, get the text only from within that article tag\n",
    "        article_text = article_tag.get_text(strip=True, separator=' ')\n",
    "        \n",
    "        # Print the first 500 characters of the article text\n",
    "        print(article_text[:500])\n",
    "    else:\n",
    "        print(\"Could not find an <article> tag in the HTML.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {html_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model 'en_core_web_sm'...\n",
      "Processing text and counting token frequencies...\n",
      "\n",
      "--- Analysis Results ---\n",
      "\n",
      "The 5 most frequent tokens are:\n",
      "['laser', 'headlights', 'headlight', 'technology', 'led']\n",
      "\n",
      "Frequency of the 5 most common tokens:\n",
      "- 'laser': 35\n",
      "- 'headlights': 19\n",
      "- 'headlight': 11\n",
      "- 'technology': 10\n",
      "- 'led': 10\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_article_text():\n",
    "    \"\"\"\n",
    "    Loads an HTML article, extracts the text, and uses spaCy to find the\n",
    "    most frequent tokens.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the HTML file.\n",
    "    \"\"\"\n",
    "    # --- 1. HTML file was loaded in previous code block ---\n",
    "\n",
    "    # --- 2. Load spaCy Model ---\n",
    "    print(\"Loading spaCy model 'en_core_web_sm'...\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\n",
    "            \"spaCy model 'en_core_web_sm' not found. Please run:\\n\"\n",
    "            \"python -m spacy download en_core_web_sm\",\n",
    "            file=sys.stderr\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Process Text and Filter Tokens ---\n",
    "    print(\"Processing text and counting token frequencies...\")\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Create a list of tokens, converted to lower case,\n",
    "    # but only if they are not stopwords, punctuation, or whitespace.\n",
    "    filtered_tokens = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "\n",
    "    # --- 4. Count Frequencies and Find Most Common ---\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    most_common_tokens = word_freq.most_common(5)\n",
    "\n",
    "    # --- 5. Print the Results ---\n",
    "    print(\"\\n--- Analysis Results ---\")\n",
    "    \n",
    "    # Print the list of the 5 most common tokens\n",
    "    common_token_list = [token for token, freq in most_common_tokens]\n",
    "    print(\"\\nThe 5 most frequent tokens are:\")\n",
    "    print(common_token_list)\n",
    "\n",
    "    # Print the tokens and their frequencies\n",
    "    print(\"\\nFrequency of the 5 most common tokens:\")\n",
    "    for token, freq in most_common_tokens:\n",
    "        print(f\"- '{token}': {freq}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "analyze_article_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
